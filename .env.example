# =============================================================================
# PPTX POC - Docker Compose Environment Configuration (Example)
# =============================================================================
# Copy this file to .env and modify as needed for your environment.
#
#   cp .env.example .env
#
# Ports are set in the 5xxx range by default to avoid conflicts with common
# services. Adjust these if you need different ports.
# =============================================================================

# -----------------------------------------------------------------------------
# SERVICE PORTS (Host:Container mapping - only host ports are configurable)
# -----------------------------------------------------------------------------

# Orchestrator API (main backend) - access at http://localhost:5100
ORCHESTRATOR_PORT=5100

# PPTX Generator service - access at http://localhost:5101
PPTX_GENERATOR_PORT=5101

# Frontend (Nginx serving static files) - access at http://localhost:5102
FRONTEND_PORT=5102

# -----------------------------------------------------------------------------
# OLLAMA CONFIGURATION
# -----------------------------------------------------------------------------
# The main stack (docker-compose.yml) connects to an EXTERNAL Ollama instance
# via Docker network. This avoids duplicating models and allows you to use
# an existing Ollama installation.
#
# If you don't have Ollama running:
#   docker-compose -f docker-compose.ollama.yml up -d
#   docker exec ollama ollama pull ministral-3:14b-instruct-2512-q8_0

# Name of the external Docker network where Ollama is running
OLLAMA_NETWORK=ollama-network

# Hostname of the Ollama container (as seen within the Docker network)
OLLAMA_CONTAINER_NAME=ollama

# Ollama API port (internal, not exposed - used for service communication)
OLLAMA_PORT=11434

# Full Ollama host URL for services to connect
OLLAMA_HOST=http://ollama:11434

# External port for Ollama (only used in docker-compose.ollama.yml)
# Set to 5199 to avoid conflict with existing Ollama on 11434
OLLAMA_EXTERNAL_PORT=5199

# -----------------------------------------------------------------------------
# MODEL CONFIGURATION
# -----------------------------------------------------------------------------
# Model to use - must already be pulled in your Ollama instance.
# NOTE: The optimal model depends on your GPU VRAM. Experiment to find what works!
#
# RECOMMENDED (24GB+ VRAM - e.g., RTX 3090/4090):
#   OLLAMA_MODEL=ministral-3-14b-it-2512-q8-120k:latest
#
# ALTERNATIVE (16GB VRAM - e.g., RTX 4080):
#   OLLAMA_MODEL=ministral-3:14b-instruct-2512-q4_K_M
#
# LIGHTER OPTIONS (8-12GB VRAM):
#   OLLAMA_MODEL=ministral-3:8b-instruct-2512-q8_0
#
# Change this to match your available Ollama model:
OLLAMA_MODEL=ministral-3-14b-it-2512-q8-120k:latest

# -----------------------------------------------------------------------------
# OLLAMA GENERATION PARAMETERS (Defaults - can be overridden via frontend)
# -----------------------------------------------------------------------------
# Temperature: Controls randomness (0.0 = deterministic, 2.0 = very random)
# Lower values are better for consistent JSON output
OLLAMA_TEMPERATURE=0.15

# Context window size: How much text the model can "see" at once
# Larger = more capable but uses more VRAM. Max depends on your model/GPU.
OLLAMA_NUM_CTX=122880

# Request timeout in seconds (large models can take a while)
OLLAMA_TIMEOUT=120
